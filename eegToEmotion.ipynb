{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys, subprocess, pathlib, re\n",
        "\n",
        "def sh(cmd):\n",
        "    print(\">>\", cmd)\n",
        "    subprocess.check_call(cmd, shell=True)\n",
        "\n",
        "sh(f\"{sys.executable} -m pip -q install --upgrade pip setuptools wheel\")\n",
        "sh(f\"{sys.executable} -m pip -q install synapseclient scikit-learn matplotlib tqdm\")\n",
        "sh(f\"{sys.executable} -m pip -q install mne mne-connectivity xmltodict numpy scipy pandas joblib\")\n",
        "sh(f\"{sys.executable} -m pip -q uninstall -y torcheeg || true\")\n",
        "sh(\"rm -rf torcheeg_src\")\n",
        "sh(\"git clone --depth 1 --branch v1.1.3 https://github.com/torcheeg/torcheeg.git torcheeg_src\")\n",
        "\n",
        "setup_py = pathlib.Path(\"torcheeg_src/setup.py\")\n",
        "txt = setup_py.read_text()\n",
        "\n",
        "# Remove scipy<=1.10.1 constraint\n",
        "txt2 = re.sub(r\"scipy>=1\\.7\\.3\\s*,\\s*<=\\s*1\\.10\\.1\", \"scipy>=1.7.3\", txt)\n",
        "setup_py.write_text(txt2)\n",
        "\n",
        "# now install with deps\n",
        "sh(f\"{sys.executable} -m pip -q install ./torcheeg_src\")"
      ],
      "metadata": {
        "id": "Tc7Chg7It8w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "SEED = 0\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "ROOT = Path(\"./FACED_torcheeg\")\n",
        "DL   = ROOT/\"downloads\"\n",
        "DATA = ROOT/\"data\"\n",
        "DL.mkdir(parents=True, exist_ok=True)\n",
        "DATA.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EMO = [\"anger\",\"disgust\",\"fear\",\"sadness\",\"neutral\",\"amusement\",\"inspiration\",\"joy\",\"tenderness\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS58CkthuiiM",
        "outputId": "c8797567-7ac5-49c0-e260-f0a79f1c1a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, getpass, synapseclient, os\n",
        "\n",
        "SYN_EEG_FEATURES = \"syn52368847\"\n",
        "\n",
        "token = os.environ.get(\"SYNAPSE_AUTH_TOKEN\")\n",
        "if not token:\n",
        "    token = getpass.getpass(\"Synapse Personal Access Token: \").strip()\n",
        "\n",
        "syn = synapseclient.Synapse()\n",
        "syn.login(authToken=token, silent=True)\n",
        "\n",
        "ent = syn.get(SYN_EEG_FEATURES, downloadLocation=str(DL))\n",
        "feat_zip = Path(ent.path)\n",
        "print(\"Downloaded:\", feat_zip)\n",
        "\n",
        "feat_dir = DATA/\"EEG_Features_unzipped\"\n",
        "feat_dir.mkdir(parents=True, exist_ok=True)\n",
        "marker = feat_dir/\".unzipped_ok\"\n",
        "\n",
        "if not marker.exists():\n",
        "    with zipfile.ZipFile(feat_zip, \"r\") as z:\n",
        "        z.extractall(feat_dir)\n",
        "    marker.write_text(\"ok\")\n",
        "\n",
        "cands = [\n",
        "    feat_dir/\"EEG_Features\"/\"DE\",\n",
        "    feat_dir/\"DE\",\n",
        "]\n",
        "DE_path = next((p for p in cands if p.exists()), None)\n",
        "print(\"DE_path:\", DE_path)\n",
        "assert DE_path is not None, \"Couldn't find EEG_Features/DE after unzip.\""
      ],
      "metadata": {
        "id": "cMm_QOiluj0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torcheeg.datasets import FACEDFeatureDataset\n",
        "from torcheeg import transforms\n",
        "from torcheeg.datasets.constants import FACED_CHANNEL_LOCATION_DICT\n",
        "\n",
        "IO_PATH = str((DATA/\"io_faced_de\").resolve())\n",
        "\n",
        "dataset = FACEDFeatureDataset(\n",
        "    root_path=str(DE_path),\n",
        "    offline_transform=transforms.ToGrid(FACED_CHANNEL_LOCATION_DICT),\n",
        "    online_transform=transforms.ToTensor(),\n",
        "    label_transform=transforms.Select(\"emotion\"),\n",
        "    io_mode=\"pickle\",\n",
        "    io_path=IO_PATH,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"len(dataset) =\", len(dataset))\n",
        "\n",
        "item = dataset[0]\n",
        "print(\"len(dataset[0]) =\", len(item))\n",
        "print(\"type(dataset[0][0]) =\", type(item[0]), \"shape =\", tuple(item[0].shape))\n",
        "print(\"label =\", int(item[1]))"
      ],
      "metadata": {
        "id": "IhNeLv0fuls_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "N = len(dataset)\n",
        "\n",
        "try:\n",
        "    labels = np.fromiter((int(dataset.read_info(i)[\"emotion\"]) for i in range(N)),\n",
        "                         dtype=np.int64, count=N)\n",
        "except Exception as e:\n",
        "    print(\"read_info failed, falling back to dataset[i][1] labels:\", e)\n",
        "    labels = np.fromiter((int(dataset[i][1]) for i in range(N)),\n",
        "                         dtype=np.int64, count=N)\n",
        "\n",
        "idx = np.arange(N)\n",
        "\n",
        "idx_train, idx_tmp, y_train, y_tmp = train_test_split(\n",
        "    idx, labels, test_size=0.2, random_state=SEED, stratify=labels\n",
        ")\n",
        "idx_val, idx_test, y_val, y_test = train_test_split(\n",
        "    idx_tmp, y_tmp, test_size=0.5, random_state=SEED, stratify=y_tmp\n",
        ")\n",
        "\n",
        "print(\"splits:\", len(idx_train), len(idx_val), len(idx_test))\n",
        "print(\"train class counts:\", np.bincount(y_train, minlength=9))\n",
        "\n",
        "class Wrap(Dataset):\n",
        "    def __init__(self, base, indices):\n",
        "        self.base = base\n",
        "        self.indices = np.asarray(indices)\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    def __getitem__(self, i):\n",
        "        x, y = self.base[int(self.indices[i])]\n",
        "        x = x.float()\n",
        "        # per-sample normalization\n",
        "        mu = x.mean(dim=(1,2), keepdim=True)\n",
        "        sd = x.std(dim=(1,2), keepdim=True).clamp_min(1e-6)\n",
        "        x = (x - mu) / sd\n",
        "        return x, int(y)\n",
        "\n",
        "train_loader = DataLoader(Wrap(dataset, idx_train), batch_size=256, shuffle=True,  num_workers=0)\n",
        "val_loader   = DataLoader(Wrap(dataset, idx_val),   batch_size=512, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(Wrap(dataset, idx_test),  batch_size=512, shuffle=False, num_workers=0)\n",
        "\n",
        "xb, yb = next(iter(train_loader))\n",
        "print(\"batch:\", xb.shape, yb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5FQ-QxnunNB",
        "outputId": "80e2d617-3d60-401d-99d7-ef6cddbe3838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "splits: 82656 10332 10332\n",
            "train class counts: [ 8856  8856  8856  8856 11808  8856  8856  8856  8856]\n",
            "batch: torch.Size([256, 5, 8, 9]) torch.Size([256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "in_ch = xb.shape[1]\n",
        "H, W  = xb.shape[-2], xb.shape[-1]\n",
        "\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes=9):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = SmallCNN(in_channels=in_ch, num_classes=9).to(device)\n",
        "\n",
        "counts = np.bincount(y_train, minlength=9).astype(np.float32)\n",
        "weights = (counts.sum() / (counts + 1e-6))\n",
        "weights = weights / weights.mean()\n",
        "class_w = torch.tensor(weights, dtype=torch.float32, device=device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_w)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "\n",
        "print(\"model ok | in_ch:\", in_ch, \"| grid:\", (H, W))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSU-mUlkuqyu",
        "outputId": "79ea701c-643f-4571-97d3-d5740e147055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model ok | in_ch: 5 | grid: (8, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "    model.train(train)\n",
        "    total_loss, total_correct, total_n = 0.0, 0, 0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += float(loss.item()) * len(yb)\n",
        "        total_correct += int((logits.argmax(1) == yb).sum().item())\n",
        "        total_n += len(yb)\n",
        "\n",
        "    return total_loss / total_n, total_correct / total_n\n",
        "\n",
        "\n",
        "MAX_EPOCHS = 200\n",
        "PATIENCE   = 12       # stop after this many epochs w/o val improvement\n",
        "MIN_DELTA  = 1e-3\n",
        "MIN_EPOCHS = 10\n",
        "\n",
        "trL, vaL, trA, vaA = [], [], [], []\n",
        "best_val_loss, best_state, best_epoch = float(\"inf\"), None, -1\n",
        "bad_epochs = 0\n",
        "\n",
        "for e in range(1, MAX_EPOCHS + 1):\n",
        "    tl, ta = run_epoch(train_loader, True)\n",
        "    vl, va = run_epoch(val_loader, False)\n",
        "\n",
        "    trL.append(tl); trA.append(ta)\n",
        "    vaL.append(vl); vaA.append(va)\n",
        "\n",
        "    improved = (vl < best_val_loss - MIN_DELTA)\n",
        "    if improved:\n",
        "        best_val_loss = vl\n",
        "        best_epoch = e\n",
        "        bad_epochs = 0\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "\n",
        "    print(f\"epoch {e:03d} | train loss {tl:.4f} acc {ta:.3f} | val loss {vl:.4f} acc {va:.3f} \"\n",
        "          f\"| best val loss {best_val_loss:.4f} @ {best_epoch:03d} | patience {bad_epochs}/{PATIENCE}\")\n",
        "\n",
        "    if e >= MIN_EPOCHS and bad_epochs >= PATIENCE:\n",
        "        print(f\"Early stopping: no val loss improvement for {PATIENCE} epochs. Stopping at epoch {e}.\")\n",
        "        break\n",
        "\n",
        "# restore best (lowest val loss)\n",
        "model.load_state_dict(best_state)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(trL, label=\"train loss\")\n",
        "plt.plot(vaL, label=\"val loss\")\n",
        "plt.axvline(best_epoch-1, linestyle=\"--\", label=\"best epoch (val loss)\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss curves\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(trA, label=\"train acc\")\n",
        "plt.plot(vaA, label=\"val acc\")\n",
        "plt.axvline(best_epoch-1, linestyle=\"--\", label=\"best epoch (val loss)\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy curves\")\n",
        "plt.show()\n",
        "\n",
        "print(\"best val loss:\", best_val_loss, \"at epoch\", best_epoch)\n",
        "print(\"val acc at best loss epoch:\", vaA[best_epoch-1])"
      ],
      "metadata": {
        "id": "9MATh1tgus0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "model.eval()\n",
        "preds, trues = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        preds.append(logits.argmax(1).cpu().numpy())\n",
        "        trues.append(np.asarray(yb))\n",
        "\n",
        "y_pred = np.concatenate(preds)\n",
        "y_true = np.concatenate(trues)\n",
        "\n",
        "print(\"Test acc:\", accuracy_score(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=EMO, digits=3, zero_division=0))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=np.arange(9))\n",
        "plt.figure(figsize=(7,6))\n",
        "plt.imshow(cm)\n",
        "plt.title(\"Confusion matrix (test)\")\n",
        "plt.xlabel(\"pred\"); plt.ylabel(\"true\")\n",
        "plt.xticks(range(9), EMO, rotation=45, ha=\"right\")\n",
        "plt.yticks(range(9), EMO)\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SdDryJbbvCSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time, hashlib\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "if \"best_state\" in globals() and best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "meta = {\n",
        "    \"saved_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "    \"task\": \"EEG emotion classification (FACED DE -> 2D scalp grid -> CNN)\",\n",
        "    \"classes\": EMO if \"EMO\" in globals() else None,\n",
        "    \"num_classes\": len(EMO) if \"EMO\" in globals() else None,\n",
        "    \"input_tensor_shape\": [1, 5, 8, 9],   # (B, BANDS, H, W) for inference\n",
        "    \"bands\": 5,\n",
        "    \"grid_hw\": [8, 9],\n",
        "    \"notes\": \"Checkpoint contains model.state_dict only; recreate architecture in inference code before loading.\"\n",
        "}\n",
        "\n",
        "out_dir = Path(\"export_ckpt\")\n",
        "out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "ckpt_path = out_dir / \"cnn_faced_best.pt\"\n",
        "meta_path = out_dir / \"cnn_faced_best.meta.json\"\n",
        "\n",
        "torch.save(\n",
        "    {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"meta\": meta,\n",
        "    },\n",
        "    ckpt_path\n",
        ")\n",
        "\n",
        "meta_path.write_text(json.dumps(meta, indent=2))\n",
        "\n",
        "sha256 = hashlib.sha256(ckpt_path.read_bytes()).hexdigest()\n",
        "(out_dir / \"sha256.txt\").write_text(sha256 + \"\\n\")\n",
        "\n",
        "print(\"Saved checkpoint:\", ckpt_path)\n",
        "print(\"Saved metadata:\", meta_path)\n",
        "print(\"SHA256:\", sha256)\n",
        "\n",
        "import zipfile\n",
        "zip_path = Path(\"cnn_faced_export_loss_stop.zip\")\n",
        "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    z.write(ckpt_path, arcname=ckpt_path.name)\n",
        "    z.write(meta_path, arcname=meta_path.name)\n",
        "    z.write(out_dir / \"sha256.txt\", arcname=\"sha256.txt\")\n",
        "\n",
        "print(\"Zipped:\", zip_path)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(str(zip_path))"
      ],
      "metadata": {
        "id": "u7GbNBJezv3A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}